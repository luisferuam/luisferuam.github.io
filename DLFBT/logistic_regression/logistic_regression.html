<!DOCTYPE html>
<!--
==============================================================================
           "GitHub HTML5 Pandoc Template" v2.2 — by Tristano Ajmone
==============================================================================
Copyright © Tristano Ajmone, 2017-2020, MIT License (MIT). Project's home:

- https://github.com/tajmone/pandoc-goodies

The CSS in this template reuses source code taken from the following projects:

- GitHub Markdown CSS: Copyright © Sindre Sorhus, MIT License (MIT):
  https://github.com/sindresorhus/github-markdown-css

- Primer CSS: Copyright © 2016-2017 GitHub Inc., MIT License (MIT):
  http://primercss.io/

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The MIT License

Copyright (c) Tristano Ajmone, 2017-2020 (github.com/tajmone/pandoc-goodies)
Copyright (c) Sindre Sorhus <sindresorhus@gmail.com> (sindresorhus.com)
Copyright (c) 2017 GitHub Inc.

"GitHub Pandoc HTML5 Template" is Copyright (c) Tristano Ajmone, 2017-2020,
released under the MIT License (MIT); it contains readaptations of substantial
portions of the following third party softwares:

(1) "GitHub Markdown CSS", Copyright (c) Sindre Sorhus, MIT License (MIT).
(2) "Primer CSS", Copyright (c) 2016 GitHub Inc., MIT License (MIT).

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
==============================================================================-->
<html>
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Luis F. Lago-Fernández" />
  <meta name="dcterms.date" content="2022-09-08" />
  <title>Logistic Regression</title>
  <style type="text/css">
@charset "UTF-8";.markdown-body{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;color:#24292e;font-family:-apple-system,system-ui,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";font-size:16px;line-height:1.5;word-wrap:break-word;box-sizing:border-box;min-width:200px;max-width:980px;margin:0 auto;padding:45px}.markdown-body a{color:#0366d6;background-color:transparent;text-decoration:none;-webkit-text-decoration-skip:objects}.markdown-body a:active,.markdown-body a:hover{outline-width:0}.markdown-body a:hover{text-decoration:underline}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body strong{font-weight:600}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1{font-size:2em;margin:.67em 0;padding-bottom:.3em;border-bottom:1px solid #eaecef}.markdown-body h2{padding-bottom:.3em;font-size:1.5em;border-bottom:1px solid #eaecef}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#6a737d}.markdown-body img{border-style:none}.markdown-body svg:not(:root){overflow:hidden}.markdown-body hr{box-sizing:content-box;height:.25em;margin:24px 0;padding:0;overflow:hidden;background-color:#e1e4e8;border:0}.markdown-body hr::before{display:table;content:""}.markdown-body hr::after{display:table;clear:both;content:""}.markdown-body input{margin:0;overflow:visible;font:inherit;font-family:inherit;font-size:inherit;line-height:inherit}.markdown-body [type=checkbox]{box-sizing:border-box;padding:0}.markdown-body *{box-sizing:border-box}.markdown-body blockquote{margin:0}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol ol,.markdown-body ul ol{list-style-type:lower-roman}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body ol ol ol,.markdown-body ol ul ol,.markdown-body ul ol ol,.markdown-body ul ul ol{list-style-type:lower-alpha}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dd{margin-left:0}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:600}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body code{font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace}.markdown-body pre{font:12px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;word-wrap:normal}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body blockquote{padding:0 1em;color:#6a737d;border-left:.25em solid #dfe2e5}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body table{display:block;width:100%;overflow:auto;border-spacing:0;border-collapse:collapse}.markdown-body table th{font-weight:600}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #dfe2e5}.markdown-body table tr{background-color:#fff;border-top:1px solid #c6cbd1}.markdown-body table tr:nth-child(2n){background-color:#f6f8fa}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body code{padding:.2em 0;margin:0;font-size:90%;background-color:rgba(27,31,35,.05);border-radius:3px}.markdown-body code::after,.markdown-body code::before{letter-spacing:-.2em;content:" "}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:90%;line-height:1.45;background-color:#f6f8fa;border-radius:3px}.markdown-body pre code{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code::after,.markdown-body pre code::before{content:normal}.markdown-body .full-commit .btn-outline:not(:disabled):hover{color:#005cc5;border-color:#005cc5}.markdown-body kbd{box-shadow:inset 0 -1px 0 #959da5;display:inline-block;padding:3px 5px;font:11px/10px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;color:#444d56;vertical-align:middle;background-color:#fcfcfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}.markdown-body :checked+.radio-label{position:relative;z-index:1;border-color:#0366d6}.markdown-body .task-list-item{list-style-type:none}.markdown-body .task-list-item+.task-list-item{margin-top:3px}.markdown-body .task-list-item input{margin:0 .2em .25em -1.6em;vertical-align:middle}.markdown-body::before{display:table;content:""}.markdown-body::after{display:table;clear:both;content:""}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.Alert,.Error,.Note,.Success,.Warning{padding:11px;margin-bottom:24px;border-style:solid;border-width:1px;border-radius:4px}.Alert p,.Error p,.Note p,.Success p,.Warning p{margin-top:0}.Alert p:last-child,.Error p:last-child,.Note p:last-child,.Success p:last-child,.Warning p:last-child{margin-bottom:0}.Alert{color:#246;background-color:#e2eef9;border-color:#bac6d3}.Warning{color:#4c4a42;background-color:#fff9ea;border-color:#dfd8c2}.Error{color:#911;background-color:#fcdede;border-color:#d2b2b2}.Success{color:#22662c;background-color:#e2f9e5;border-color:#bad3be}.Note{color:#2f363d;background-color:#f6f8fa;border-color:#d5d8da}.Alert h1,.Alert h2,.Alert h3,.Alert h4,.Alert h5,.Alert h6{color:#246;margin-bottom:0}.Warning h1,.Warning h2,.Warning h3,.Warning h4,.Warning h5,.Warning h6{color:#4c4a42;margin-bottom:0}.Error h1,.Error h2,.Error h3,.Error h4,.Error h5,.Error h6{color:#911;margin-bottom:0}.Success h1,.Success h2,.Success h3,.Success h4,.Success h5,.Success h6{color:#22662c;margin-bottom:0}.Note h1,.Note h2,.Note h3,.Note h4,.Note h5,.Note h6{color:#2f363d;margin-bottom:0}.Alert h1:first-child,.Alert h2:first-child,.Alert h3:first-child,.Alert h4:first-child,.Alert h5:first-child,.Alert h6:first-child,.Error h1:first-child,.Error h2:first-child,.Error h3:first-child,.Error h4:first-child,.Error h5:first-child,.Error h6:first-child,.Note h1:first-child,.Note h2:first-child,.Note h3:first-child,.Note h4:first-child,.Note h5:first-child,.Note h6:first-child,.Success h1:first-child,.Success h2:first-child,.Success h3:first-child,.Success h4:first-child,.Success h5:first-child,.Success h6:first-child,.Warning h1:first-child,.Warning h2:first-child,.Warning h3:first-child,.Warning h4:first-child,.Warning h5:first-child,.Warning h6:first-child{margin-top:0}h1.title,p.subtitle{text-align:center}h1.title.followed-by-subtitle{margin-bottom:0}p.subtitle{font-size:1.5em;font-weight:600;line-height:1.25;margin-top:0;margin-bottom:16px;padding-bottom:.3em}div.line-block{white-space:pre-line}
  </style>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<article class="markdown-body">
<header>
<h1 class="title"><font color="#CA3532">Logistic Regression</font></h1>
<p class="author">Luis F. Lago-Fernández</p>
<p class="date">September 08, 2022</p>
<a href="https://colab.research.google.com/github/luisferuam/luisferuam.github.io/blob/master/DLFBT/logistic_regression/logistic_regression.ipynb">Open in Google Colab</a>
</header>
<hr>
<nav id="TOC">
<h2 class="toc-title"><font color="#CA3532">Contents</font></h2>
<ul>
<li><a href="#introduction"><font color="#CA3532">Introduction</font></a></li>
<li><a href="#cross-entropy-loss"><font color="#CA3532">Cross-entropy loss</font></a></li>
<li><a href="#logistic-regression-in-1d"><font color="#CA3532">Logistic regression in 1D</font></a></li>
<li><a href="#gradient-descent"><font color="#CA3532">Gradient descent</font></a>
<ul>
<li><a href="#questions"><font color="#CA3532">Questions</font></a></li>
</ul></li>
<li><a href="#logistic-regression-in-arbitrary-dimension"><font color="#CA3532">Logistic regression in arbitrary dimension</font></a>
<ul>
<li><a href="#exercise"><font color="#CA3532">Exercise</font></a></li>
</ul></li>
</ul>
</nav>
<hr>
<h2 id="introduction"><font color="#CA3532">Introduction</font></h2>
<p>Similarly to the linear regression case, let's assume that we have a set of pairs <span class="math inline">\(({\bf x}_{i}, t_{i})\)</span>, where <span class="math inline">\({\bf x}_{i} \in \mathbb{R}^{d}\)</span> is a <span class="math inline">\(d\)</span>-dimensional attribute vector and <span class="math inline">\(t_{i}\)</span> is the target variable. The difference is that now <span class="math inline">\(t_{i}\)</span> can take only two different values, <span class="math inline">\(t_{i} \in \{0, 1\}\)</span>. As before, we want to predict the value of <span class="math inline">\(t_{i}\)</span> from <span class="math inline">\({\bf x}_{i}\)</span>. Now we use the following model:</p>
<p><span class="math display">\[y_{i} = \sigma({\bf w}^{t} {\bf x}_{i} + b).\]</span></p>
<p>This is basically a linear model <span class="math inline">\(z = {\bf w}^{t} {\bf x} + b\)</span> followed by application of the <strong>sigmoid</strong> or <strong>logistic</strong> function:</p>
<p><span class="math display">\[\sigma(z) = \frac{e^{z}}{e^{z}+1} = \frac{1}{1+e^{-z}}.\]</span></p>
<p>The logistic function maps its input to the range <span class="math inline">\([0, 1]\)</span>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="fl">0.01</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>plt.plot(z, y)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;z&quot;</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;$\sigma(z)$&quot;</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;logistic function&quot;</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="myMediaFolder/0986702268ef95519856abf9bb812d916a429ed8.png" /></p>
<p>We usually interpret the model output <span class="math inline">\(y_{i} = \sigma({\bf w}^{t} {\bf x}_{i} + b)\)</span> as the probability of the target being <span class="math inline">\(1\)</span> given the attribute vector:</p>
<p><span class="math display">\[
y_{i} = P(t = 1 | {\bf x}_{i}).
\]</span></p>
<p>And hence:</p>
<p><span class="math display">\[
1 - y_{i} = P(t = 0 | {\bf x}_{i}).
\]</span></p>
<p>So our objective is now to find a set of parameters <span class="math inline">\(\theta = \{{\bf w}, b\}\)</span> that maximizes the likelihood of the targets <span class="math inline">\(t_{i}\)</span> given the attributes <span class="math inline">\({\bf x}_{i}\)</span>.</p>
<h2 id="cross-entropy-loss"><font color="#CA3532">Cross-entropy loss</font></h2>
<p>The likelihood is given by:</p>
<p><span class="math display">\[l = \prod_{i} P(t = t_{i} | {\bf x}_{i}).\]</span></p>
<p>Taking logarithms we obtain the log-likelihood <span class="math inline">\(L\)</span>:</p>
<p><span class="math display">\[L = \log{l} = \sum_{i} \log{P(t = t_{i} | {\bf x}_{i})}.\]</span></p>
<p>And substituting the model estimation of <span class="math inline">\(P(t_{i} | {\bf x}_{i})\)</span> we get:</p>
<p><span class="math display">\[L = \sum_{i} [t_{i} \log y_{i} + (1-t_{i}) \log (1-y_{i})],\]</span></p>
<p>where the trick is to realize that only one of the two terms in the sum above is not zero. In order to obtain a loss function that we minimize, we simply add a minus sign to define the <strong>cross-entropy</strong> loss as:</p>
<p><span class="math display">\[C = -\sum_{i} [t_{i} \log y_{i} + (1-t_{i}) \log (1-y_{i})].\]</span></p>
<p>We solve the logistic regression model by finding the parameters <span class="math inline">\(({\bf w}, b)\)</span> that minimize the cross-entropy loss. Contrary to linear regression, logistic regression does not have an analytic solution and must be solved numerically, using for example <strong>gradient descent</strong>.</p>
<h2 id="logistic-regression-in-1d"><font color="#CA3532">Logistic regression in 1D</font></h2>
<p>As we did for linear regression, we will first consider a simple example in 1D, where <span class="math inline">\(x_{i}\)</span> is a scalar. In this case the model is simply:</p>
<p><span class="math display">\[y_{i} = \sigma (w x_{i} + b).\]</span></p>
<p>The following code creates a set of 1000 data points <span class="math inline">\((x_{i}, t_{i})\)</span> with <span class="math inline">\(x_{i} \in [0, 10]\)</span>. Given <span class="math inline">\(x_{i}\)</span>, we randomly determine the value of <span class="math inline">\(t_{i}\)</span> assuming that <span class="math inline">\(P(t_{i} = 1 | {\bf x}_{i}) = \sigma(2x_{i} - 10)\)</span>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters:</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="op">-</span><span class="fl">10.0</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>xmin <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>xmax <span class="op">=</span> <span class="fl">10.0</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly generated problem data:</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> xmin <span class="op">+</span> np.random.rand(n)<span class="op">*</span>(xmax <span class="op">-</span> xmin)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> a<span class="op">*</span>x <span class="op">+</span> b</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> <span class="fl">1.0</span><span class="op">/</span>(<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>z)) <span class="op">&gt;</span> np.random.rand(n)</span></code></pre></div>
<p>The following code shows how the data are distributed in the plane <span class="math inline">\((x, t)\)</span>, together with the true probability <span class="math inline">\(P(t = 1 | x) = \sigma(2x - 10)\)</span>.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>plt.plot(x, t, <span class="st">&#39;o&#39;</span>, label<span class="op">=</span><span class="st">&#39;data points&#39;</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="bu">xrange</span> <span class="op">=</span> np.arange(xmin, xmax, <span class="fl">0.1</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>realt <span class="op">=</span> <span class="fl">1.0</span><span class="op">/</span>(<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>a<span class="op">*</span><span class="bu">xrange</span> <span class="op">-</span> b))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">xrange</span>, realt, <span class="st">&#39;r-&#39;</span>, label<span class="op">=</span><span class="st">&#39;true prob $P(t = 1 | x)$&#39;</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;x&quot;</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;t&quot;</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="myMediaFolder/b1438be88035bae0ff84328013ef26a289a85675.png" /></p>
<h2 id="gradient-descent"><font color="#CA3532">Gradient descent</font></h2>
<p>In order to minimize the cross-entropy loss using gradient descent, we need to compute the gradient of the loss function with respect to the model parameters. The partial derivatives of <span class="math inline">\(C\)</span> with respect to <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> can be shown to be:</p>
<p><span class="math display">\[\frac{\partial C}{\partial w} = \sum_{i}(y_{i} - t_{i})x_{i},\]</span></p>
<p><span class="math display">\[\frac{\partial C}{\partial b} = \sum_{i}(y_{i} - t_{i}),\]</span></p>
<p>where we have made use of the fact that <span class="math inline">\(\sigma&#39;(z) = \sigma(z) (1 - \sigma(z))\)</span>. Note that the above expressions are exactly the same as those obtained for the linear regression case, and so the update rules for <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> are again:</p>
<p><span class="math display">\[w_{t+1} = w_{t} - \eta \sum_{i}(y_{i} - t_{i})x_{i},\]</span></p>
<p><span class="math display">\[b_{t+1} = b_{t} - \eta  \sum_{i}(y_{i} - t_{i}).\]</span></p>
<p>Let us apply this model to our problem data. The first thing is to initialize the model parameters <span class="math inline">\((w, b)\)</span> to random values:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.random.randn()</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.random.randn()</span></code></pre></div>
<p>We don't expect that this initial random guess provides any good estimation of the class probabilities, as demonstrated by the following piece of code:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>plt.plot(x, t, <span class="st">&#39;o&#39;</span>, label<span class="op">=</span><span class="st">&#39;data points&#39;</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">xrange</span>, realt, <span class="st">&#39;r-&#39;</span>, label<span class="op">=</span><span class="st">&#39;true prob $P(t = 1 | x)$&#39;</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>modely <span class="op">=</span> <span class="fl">1.0</span><span class="op">/</span>(<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>w<span class="op">*</span><span class="bu">xrange</span> <span class="op">-</span> b))</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">xrange</span>, modely, <span class="st">&#39;m-&#39;</span>, label<span class="op">=</span><span class="st">&#39;model&#39;</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;x&quot;</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;t&quot;</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="myMediaFolder/821dfe251ad3a96e9b337a2ee229bb1367a11c92.png" /></p>
<p>We need to perform several gradient descent steps until the algorithm converges to a good solution:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>num_iters <span class="op">=</span> <span class="dv">1000</span> <span class="co"># number of iterations</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.0002</span>     <span class="co"># learning rate</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_iters):</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> w<span class="op">*</span>x <span class="op">+</span> b</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="fl">1.0</span><span class="op">/</span>(<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    y_minus_t <span class="op">=</span> y <span class="op">-</span> t</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    dw <span class="op">=</span> np.<span class="bu">sum</span>(y_minus_t<span class="op">*</span>x)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    db <span class="op">=</span> np.<span class="bu">sum</span>(y_minus_t)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    w <span class="op">-=</span> eta<span class="op">*</span>dw </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    b <span class="op">-=</span> eta<span class="op">*</span>db </span></code></pre></div>
<p>If we plot the new solution we observe a much better approximation of the true probability by the model:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>plt.plot(x, t, <span class="st">&#39;o&#39;</span>, label<span class="op">=</span><span class="st">&#39;data points&#39;</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">xrange</span>, realt, <span class="st">&#39;r-&#39;</span>, label<span class="op">=</span><span class="st">&#39;true prob $P(t = 1 | x)$&#39;</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>modely <span class="op">=</span> <span class="fl">1.0</span><span class="op">/</span>(<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>w<span class="op">*</span><span class="bu">xrange</span> <span class="op">-</span> b))</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">xrange</span>, modely, <span class="st">&#39;m-&#39;</span>, label<span class="op">=</span><span class="st">&#39;model&#39;</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;x&quot;</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;t&quot;</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="myMediaFolder/4974a12a337b41673b1552d0e5b1e6adb68acd5c.png" /></p>
<p>It is interesting to see how the model converges to the optimal solution. The following code shows this evolution, starting from a new random guess of the parameters:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.random.randn()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.random.randn()</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>num_iters <span class="op">=</span> <span class="dv">1600</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.0002</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">12</span>))</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_iters):</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> w<span class="op">*</span>x <span class="op">+</span> b</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="fl">1.0</span><span class="op">/</span>(<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i<span class="op">%</span><span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        plt.subplot(<span class="dv">4</span>, <span class="dv">4</span>, k)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        plt.plot(x, t, <span class="st">&#39;o&#39;</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        plt.plot(<span class="bu">xrange</span>, realt, <span class="st">&#39;r-&#39;</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        modely <span class="op">=</span> <span class="fl">1.0</span><span class="op">/</span>(<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>w<span class="op">*</span><span class="bu">xrange</span> <span class="op">-</span> b))</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        plt.plot(<span class="bu">xrange</span>, modely, <span class="st">&#39;m-&#39;</span>)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>        plt.xticks([])</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>        plt.yticks([])</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">&quot;epoch = </span><span class="sc">%d</span><span class="st">&quot;</span> <span class="op">%</span> i)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        k <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    y_minus_t <span class="op">=</span> y <span class="op">-</span> t</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    dw <span class="op">=</span> np.<span class="bu">sum</span>(y_minus_t<span class="op">*</span>x)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    db <span class="op">=</span> np.<span class="bu">sum</span>(y_minus_t)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    w <span class="op">-=</span> eta<span class="op">*</span>dw</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    b <span class="op">-=</span> eta<span class="op">*</span>db</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="myMediaFolder/997318a01ebf9b7fceae1f0b7d7ab9506a323157.png" /></p>
<p>Finally, we can observe this evolution in the <span class="math inline">\((w, b)\)</span> plane:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="op">-</span><span class="fl">9.0</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>whistory <span class="op">=</span> [w]</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>bhistory <span class="op">=</span> [b]</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>num_iters <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> <span class="fl">0.0002</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_iters):</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> w<span class="op">*</span>x <span class="op">+</span> b</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="fl">1.0</span><span class="op">/</span>(<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    y_minus_t <span class="op">=</span> y <span class="op">-</span> t</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    dw <span class="op">=</span> np.<span class="bu">sum</span>(y_minus_t<span class="op">*</span>x)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    db <span class="op">=</span> np.<span class="bu">sum</span>(y_minus_t)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    w <span class="op">-=</span> eta<span class="op">*</span>dw</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    b <span class="op">-=</span> eta<span class="op">*</span>db</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    whistory.append(w)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    bhistory.append(b)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>wvalues <span class="op">=</span> np.arange(<span class="fl">0.</span>, <span class="fl">4.01</span>, <span class="fl">0.01</span>)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>bvalues <span class="op">=</span> np.arange(<span class="op">-</span><span class="fl">12.</span>, <span class="op">-</span><span class="fl">7.99</span>, <span class="fl">0.01</span>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>bgrid, wgrid <span class="op">=</span> np.meshgrid(bvalues, wvalues)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> wgrid[:, :, <span class="va">None</span>]<span class="op">*</span>x[<span class="va">None</span>, <span class="va">None</span>, :] <span class="op">+</span> bgrid[:, :, <span class="va">None</span>]</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>egrid <span class="op">=</span> np.<span class="bu">sum</span>((y<span class="op">-</span>t[<span class="va">None</span>, <span class="va">None</span>, :])<span class="op">**</span><span class="dv">2</span>, axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>plt.contourf(bgrid, wgrid, egrid, <span class="dv">40</span>, cmap<span class="op">=</span><span class="st">&#39;gray&#39;</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>plt.plot(bhistory, whistory, <span class="st">&#39;-o&#39;</span>)</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>plt.plot(bhistory[<span class="op">-</span><span class="dv">1</span>], whistory[<span class="op">-</span><span class="dv">1</span>], <span class="st">&#39;yo&#39;</span>)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;b&#39;</span>)</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;w&#39;</span>)</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="myMediaFolder/3732970a9507e21df90f871e15bc8f87de7354d5.png" /></p>
<h3 id="questions"><font color="#CA3532">Questions</font></h3>
<ul>
<li>What happens if we increase the learning rate?</li>
<li>What happens if we decrease the learning rate?</li>
</ul>
<h2 id="logistic-regression-in-arbitrary-dimension"><font color="#CA3532">Logistic regression in arbitrary dimension</font></h2>
<p>In a more general situation we have a set of data <span class="math inline">\(({\bf x}_{i}, t_{i})\)</span> with <span class="math inline">\({\bf x}_{i} \in \mathbb{R}^{d}\)</span>. The model is:</p>
<p><span class="math display">\[y_{i} = \sigma({\bf w}^{t} {\bf x}_{i} + b),\]</span></p>
<p>where <span class="math inline">\({\bf w}\)</span> is also a <span class="math inline">\(d\)</span>-dimensional vector. Now the gradients are given by the following expressions:</p>
<p><span class="math display">\[\nabla_{{\bf w}} C = \sum_{i}(y_{i} - t_{i}){\bf x}_{i},\]</span></p>
<p><span class="math display">\[\frac{\partial C}{\partial b} = \sum_{i}(y_{i} - t_{i}).\]</span></p>
<h3 id="exercise"><font color="#CA3532">Exercise</font></h3>
<ul>
<li>Extend the previous example to the multidimensional case.</li>
</ul>
</article>
</body>
</html>
