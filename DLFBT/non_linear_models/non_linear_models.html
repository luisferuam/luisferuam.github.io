<!DOCTYPE html>
<!--
==============================================================================
           "GitHub HTML5 Pandoc Template" v2.2 — by Tristano Ajmone
==============================================================================
Copyright © Tristano Ajmone, 2017-2020, MIT License (MIT). Project's home:

- https://github.com/tajmone/pandoc-goodies

The CSS in this template reuses source code taken from the following projects:

- GitHub Markdown CSS: Copyright © Sindre Sorhus, MIT License (MIT):
  https://github.com/sindresorhus/github-markdown-css

- Primer CSS: Copyright © 2016-2017 GitHub Inc., MIT License (MIT):
  http://primercss.io/

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The MIT License

Copyright (c) Tristano Ajmone, 2017-2020 (github.com/tajmone/pandoc-goodies)
Copyright (c) Sindre Sorhus <sindresorhus@gmail.com> (sindresorhus.com)
Copyright (c) 2017 GitHub Inc.

"GitHub Pandoc HTML5 Template" is Copyright (c) Tristano Ajmone, 2017-2020,
released under the MIT License (MIT); it contains readaptations of substantial
portions of the following third party softwares:

(1) "GitHub Markdown CSS", Copyright (c) Sindre Sorhus, MIT License (MIT).
(2) "Primer CSS", Copyright (c) 2016 GitHub Inc., MIT License (MIT).

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
==============================================================================-->
<html>
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Luis F. Lago-Fernández" />
  <title>Non-linear models</title>
  <style type="text/css">
@charset "UTF-8";.markdown-body{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;color:#24292e;font-family:-apple-system,system-ui,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";font-size:16px;line-height:1.5;word-wrap:break-word;box-sizing:border-box;min-width:200px;max-width:980px;margin:0 auto;padding:45px}.markdown-body a{color:#0366d6;background-color:transparent;text-decoration:none;-webkit-text-decoration-skip:objects}.markdown-body a:active,.markdown-body a:hover{outline-width:0}.markdown-body a:hover{text-decoration:underline}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body strong{font-weight:600}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1{font-size:2em;margin:.67em 0;padding-bottom:.3em;border-bottom:1px solid #eaecef}.markdown-body h2{padding-bottom:.3em;font-size:1.5em;border-bottom:1px solid #eaecef}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#6a737d}.markdown-body img{border-style:none}.markdown-body svg:not(:root){overflow:hidden}.markdown-body hr{box-sizing:content-box;height:.25em;margin:24px 0;padding:0;overflow:hidden;background-color:#e1e4e8;border:0}.markdown-body hr::before{display:table;content:""}.markdown-body hr::after{display:table;clear:both;content:""}.markdown-body input{margin:0;overflow:visible;font:inherit;font-family:inherit;font-size:inherit;line-height:inherit}.markdown-body [type=checkbox]{box-sizing:border-box;padding:0}.markdown-body *{box-sizing:border-box}.markdown-body blockquote{margin:0}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol ol,.markdown-body ul ol{list-style-type:lower-roman}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body ol ol ol,.markdown-body ol ul ol,.markdown-body ul ol ol,.markdown-body ul ul ol{list-style-type:lower-alpha}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dd{margin-left:0}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:600}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body code{font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace}.markdown-body pre{font:12px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;word-wrap:normal}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body blockquote{padding:0 1em;color:#6a737d;border-left:.25em solid #dfe2e5}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body table{display:block;width:100%;overflow:auto;border-spacing:0;border-collapse:collapse}.markdown-body table th{font-weight:600}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #dfe2e5}.markdown-body table tr{background-color:#fff;border-top:1px solid #c6cbd1}.markdown-body table tr:nth-child(2n){background-color:#f6f8fa}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body code{padding:.2em 0;margin:0;font-size:90%;background-color:rgba(27,31,35,.05);border-radius:3px}.markdown-body code::after,.markdown-body code::before{letter-spacing:-.2em;content:" "}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:90%;line-height:1.45;background-color:#f6f8fa;border-radius:3px}.markdown-body pre code{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code::after,.markdown-body pre code::before{content:normal}.markdown-body .full-commit .btn-outline:not(:disabled):hover{color:#005cc5;border-color:#005cc5}.markdown-body kbd{box-shadow:inset 0 -1px 0 #959da5;display:inline-block;padding:3px 5px;font:11px/10px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;color:#444d56;vertical-align:middle;background-color:#fcfcfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}.markdown-body :checked+.radio-label{position:relative;z-index:1;border-color:#0366d6}.markdown-body .task-list-item{list-style-type:none}.markdown-body .task-list-item+.task-list-item{margin-top:3px}.markdown-body .task-list-item input{margin:0 .2em .25em -1.6em;vertical-align:middle}.markdown-body::before{display:table;content:""}.markdown-body::after{display:table;clear:both;content:""}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.Alert,.Error,.Note,.Success,.Warning{padding:11px;margin-bottom:24px;border-style:solid;border-width:1px;border-radius:4px}.Alert p,.Error p,.Note p,.Success p,.Warning p{margin-top:0}.Alert p:last-child,.Error p:last-child,.Note p:last-child,.Success p:last-child,.Warning p:last-child{margin-bottom:0}.Alert{color:#246;background-color:#e2eef9;border-color:#bac6d3}.Warning{color:#4c4a42;background-color:#fff9ea;border-color:#dfd8c2}.Error{color:#911;background-color:#fcdede;border-color:#d2b2b2}.Success{color:#22662c;background-color:#e2f9e5;border-color:#bad3be}.Note{color:#2f363d;background-color:#f6f8fa;border-color:#d5d8da}.Alert h1,.Alert h2,.Alert h3,.Alert h4,.Alert h5,.Alert h6{color:#246;margin-bottom:0}.Warning h1,.Warning h2,.Warning h3,.Warning h4,.Warning h5,.Warning h6{color:#4c4a42;margin-bottom:0}.Error h1,.Error h2,.Error h3,.Error h4,.Error h5,.Error h6{color:#911;margin-bottom:0}.Success h1,.Success h2,.Success h3,.Success h4,.Success h5,.Success h6{color:#22662c;margin-bottom:0}.Note h1,.Note h2,.Note h3,.Note h4,.Note h5,.Note h6{color:#2f363d;margin-bottom:0}.Alert h1:first-child,.Alert h2:first-child,.Alert h3:first-child,.Alert h4:first-child,.Alert h5:first-child,.Alert h6:first-child,.Error h1:first-child,.Error h2:first-child,.Error h3:first-child,.Error h4:first-child,.Error h5:first-child,.Error h6:first-child,.Note h1:first-child,.Note h2:first-child,.Note h3:first-child,.Note h4:first-child,.Note h5:first-child,.Note h6:first-child,.Success h1:first-child,.Success h2:first-child,.Success h3:first-child,.Success h4:first-child,.Success h5:first-child,.Success h6:first-child,.Warning h1:first-child,.Warning h2:first-child,.Warning h3:first-child,.Warning h4:first-child,.Warning h5:first-child,.Warning h6:first-child{margin-top:0}h1.title,p.subtitle{text-align:center}h1.title.followed-by-subtitle{margin-bottom:0}p.subtitle{font-size:1.5em;font-weight:600;line-height:1.25;margin-top:0;margin-bottom:16px;padding-bottom:.3em}div.line-block{white-space:pre-line}
  </style>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<article class="markdown-body">
<header>
<h1 class="title"><font color="#CA3532">Non-linear models</font></h1>
<p class="author">Luis F. Lago-Fernández</p>
<p class="date"><a href="https://colab.research.google.com/github/luisferuam/luisferuam.github.io/blob/master/DLFBT/non_linear_models/non_linear_models.ipynb">Open in Google Colab</a></p>
</header>
<hr>
<nav id="TOC">
<h2 class="toc-title"><font color="#CA3532">Contents</font></h2>
<ul>
<li><a href="#neural-interpretation"><font color="#CA3532">Neural interpretation</font></a></li>
<li><a href="#the-biological-neuron"><font color="#CA3532">The biological neuron</font></a></li>
<li><a href="#the-artificial-neuron"><font color="#CA3532">The artificial neuron</font></a></li>
<li><a href="#solving-non-linear-problems"><font color="#CA3532">Solving non-linear problems</font></a></li>
<li><a href="#some-words-about-the-model-complexity"><font color="#CA3532">Some words about the model complexity</font></a></li>
<li><a href="#a-second-example"><font color="#CA3532">A second example</font></a></li>
<li><a href="#let-the-model-learn-the-representation"><font color="#CA3532">Let the model learn the representation</font></a></li>
</ul>
</nav>
<hr>
<h2 id="neural-interpretation"><font color="#CA3532">Neural interpretation</font></h2>
<p>In the previous sessions we studied two simple parametric models of supervised learning:</p>
<ul>
<li>Linear regression.</li>
<li>Logistic regression.</li>
</ul>
<p>We will see now that both models can be seen as neural networks of one single neuron.</p>
<p>An <strong>artificial neural network</strong> is a machine learning model consisting of a set of processing units called <strong>neurons</strong> interconnected in such a way that the outputs of some neurons constitute the inputs of other neurons. The final model is an abstraction of real neural circuits in animal brains.</p>
<p><img src="./myMediaFolder/3b140c4e29e0d672fb5595356720d19f724517bd.png" /></p>
<p>In the following sections we review the biological neuron, introduce the artificial neuron and relate it to the linear and logistic regression models. In the last part we show how linear models can be used to solve non linear problems.</p>
<h2 id="the-biological-neuron"><font color="#CA3532">The biological neuron</font></h2>
<p>The biological neuron is a specialized cell that is able to communicate with other neurons using electric signals. Neurons are the main components of the animal nervous system.</p>
<p>In a very simplified picture a neuron has three main elements:</p>
<ul>
<li>The <strong>dendrites</strong>, which are extensions of the cell body through which the neuron receives its inputs.</li>
<li>The <strong>soma</strong> or cell body, which accumulates all the dendritic input and <em>computes</em> the neuron's output, an electric signal that may be propagated to other neurons.</li>
<li>The <strong>axon</strong>, a long projection of the cell body that connects to the dendrites of other neurons in specific structures called <strong>synapses</strong>.</li>
</ul>
<p>The electric signal generated in response to the neuron's input is propagated along the axon and transmitted to other neurons via the synaptic connections.</p>
<h2 id="the-artificial-neuron"><font color="#CA3532">The artificial neuron</font></h2>
<p>The artificial neuron is an abstract model of the biological neuron where:</p>
<ul>
<li>The neuron's input is a <span class="math inline">\(d\)</span>-dimensional vector of real numbers that represent each of the dendritic inputs: <span class="math inline">\({\bf x} = (x_{1}, x_{2}, ..., x_{d})^{t}\)</span>.</li>
<li>The neuron computes its output by first evaluating a weighted sum of all its inputs, usually called the <strong>pre-activation</strong>:</li>
</ul>
<p><span class="math display">\[z = \sum_{i=1}^{d}w_{i}x_{i} + b,\tag{1}\]</span></p>
<ul>
<li>The weight vector <span class="math inline">\({\bf w} = (w_{1}, w_{2}, ..., w_{d})^{t}\)</span> represents the strengths of the synaptic connections associated to each of the neuron's inputs. The parameter <span class="math inline">\(b\)</span> is a special weight called the <strong>bias</strong>, which may be thought to be associated to a constant input of value <span class="math inline">\(1\)</span>.</li>
<li>The preactivation is passed through a non-linear function called the <strong>activation function</strong> to obtain the final neuron's output, or neuron's <strong>activation</strong>:</li>
</ul>
<p><span class="math display">\[y = f(z).\tag{2}\]</span></p>
<p><img src="./myMediaFolder/0a0cfaa3722ad22eaeed74d87f1a07dd18f601cc.png" /></p>
<p>We may rewrite the summatory in equation <span class="math inline">\((1)\)</span> as the dot product of the weight and input vectors to obtain a more compact expression for the neuron's activation:</p>
<p><span class="math display">\[y = f({\bf w}^{t} {\bf x} + b).\tag{3}\]</span></p>
<p>It turns out that:</p>
<ul>
<li><p>When the activation is the identity function our model neuron is equivalent to the linear regression model.</p></li>
<li><p>When the activation is the sigmoid function our model neuron is equivalent to the logistic regression model.</p></li>
</ul>
<h2 id="solving-non-linear-problems"><font color="#CA3532">Solving non-linear problems</font></h2>
<p>Sooner or later we will face a non-linear problem. For example let us consider a regression problem with 100 data <span class="math inline">\((x_{i}, t_{i})\)</span>, where <span class="math inline">\(x_{i} \in [-1, 1]\)</span> and <span class="math inline">\(t_{i}\)</span> is normally distributed around <span class="math inline">\(4x_{i}^{2} - x_{i} + 1\)</span>.</p>
<p>The following code generates a random sample with these properties.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a><span class="co"># Parameters:</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a>xmin <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a>xmax <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true"></a>noise <span class="op">=</span> <span class="fl">0.4</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true"></a><span class="co"># Randomly generated problem data:</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true"></a>x <span class="op">=</span> xmin <span class="op">+</span> np.random.rand(<span class="dv">1</span>, n)<span class="op">*</span>(xmax <span class="op">-</span> xmin)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true"></a>t <span class="op">=</span> <span class="dv">4</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> x <span class="op">+</span> <span class="dv">1</span> <span class="op">+</span> np.random.randn(n)<span class="op">*</span>noise</span></code></pre></div>
<p>And the following code shows how the data are distributed in the plane <span class="math inline">\((x, t)\)</span>, together with the model (without noise) used to generate the data.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a><span class="bu">xrange</span> <span class="op">=</span> np.arange(xmin, xmax, <span class="fl">0.01</span>)[<span class="va">None</span>, :]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true"></a>ytrue <span class="op">=</span> <span class="dv">4</span><span class="op">*</span><span class="bu">xrange</span><span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="bu">xrange</span> <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true"></a>plt.plot(x[<span class="dv">0</span>], t[<span class="dv">0</span>], <span class="st">&#39;o&#39;</span>, label<span class="op">=</span><span class="st">&quot;problem data&quot;</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true"></a>plt.plot(<span class="bu">xrange</span>[<span class="dv">0</span>], ytrue[<span class="dv">0</span>], <span class="st">&#39;r-&#39;</span>, label<span class="op">=</span><span class="st">&quot;true model&quot;</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true"></a>plt.xlabel(<span class="st">&quot;x&quot;</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true"></a>plt.ylabel(<span class="st">&quot;t&quot;</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true"></a>plt.legend()</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true"></a>plt.show()</span></code></pre></div>
<p><img src="./myMediaFolder/dcc3c60339137cd104cab230da3cabfd499c34f0.png" /></p>
<p>It is clear that a linear model will not provide a good estimation of <span class="math inline">\(t\)</span> given <span class="math inline">\(x\)</span>.</p>
<p>When facing a non-linear regression problem like the former, we may try two different approaches:</p>
<ol type="1">
<li><p>The complex way is to fit a non-linear model to our data.</p></li>
<li><p>The easy way is to transform the attribute vector <span class="math inline">\({\bf x} = (x_{1}, x_{2}, ..., x_{d})^{t}\)</span> into a new set of attributes <span class="math inline">\(\boldsymbol{\phi} ({\bf x}) = (\phi_{1}({\bf x}),\phi_{2}({\bf x}),...,\phi_{m}({\bf x}))^{t}\)</span> and keep the linear approach.</p></li>
</ol>
<p>When we follow the second approach, we make a (fixed) non-linear transformation of the attribute space, <span class="math inline">\(\boldsymbol{\phi}({\bf x})\)</span>, with the hope that the non-linearity of the problem is captured by the new attributes, while the linearity of the model keeps things being simple.</p>
<p>The new model is then:</p>
<p><span class="math display">\[
y =  w_{1} \phi_{1}({\bf x}) + w_{2} \phi_{2}({\bf x}) + ... + w_{m} \phi_{m}({\bf x}) + b= {\bf w}^{t}  \boldsymbol{\phi} ({\bf x}) + b, \tag{4}
\]</span></p>
<p>which is linear in <span class="math inline">\(\boldsymbol{\phi}({\bf x})\)</span>.</p>
<p>Going back to the example, we may select the following transformation:</p>
<p><span class="math display">\[
\boldsymbol{\phi} (x) = (x^{2}, x).\tag{5}
\]</span></p>
<p>The following code obtains the transformed attributes and trains a linear regression model on these attributes.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="co"># Generate the transformed attributes:</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a>phi <span class="op">=</span> np.concatenate((x<span class="op">**</span><span class="dv">2</span>, x))</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a>phi_range <span class="op">=</span> np.concatenate((<span class="bu">xrange</span><span class="op">**</span><span class="dv">2</span>, <span class="bu">xrange</span>))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a><span class="co"># Standardize the attributes:</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true"></a>mean_phi <span class="op">=</span> phi.mean(axis<span class="op">=</span><span class="dv">1</span>)[:, <span class="va">None</span>]</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true"></a>std_phi <span class="op">=</span> phi.std(axis<span class="op">=</span><span class="dv">1</span>)[:, <span class="va">None</span>]</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true"></a>phi <span class="op">=</span> (phi<span class="op">-</span>mean_phi)<span class="op">/</span>std_phi</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true"></a>phi_range <span class="op">=</span> (phi_range<span class="op">-</span>mean_phi)<span class="op">/</span>std_phi</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true"></a><span class="co"># Initialize model parameters:</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true"></a>w <span class="op">=</span> np.random.randn(<span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true"></a>b <span class="op">=</span> np.random.randn()</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true"></a><span class="co"># Set number of iterations and learning rate:</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true"></a>num_iters <span class="op">=</span> <span class="dv">100</span> </span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true"></a>eta <span class="op">=</span> <span class="fl">1.e-3</span>      </span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true"></a><span class="co"># Train the model:</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true"></a>loss <span class="op">=</span> []</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_iters):</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true"></a>    y <span class="op">=</span> np.dot(w.transpose(), phi) <span class="op">+</span> b</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true"></a>    y_minus_t <span class="op">=</span> y <span class="op">-</span> t</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true"></a>    loss.append(np.dot(y_minus_t, y_minus_t.transpose().ravel()))</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true"></a>    dw <span class="op">=</span> np.<span class="bu">sum</span>(y_minus_t<span class="op">*</span>phi, axis<span class="op">=</span><span class="dv">1</span>)[:, <span class="va">None</span>]</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true"></a>    db <span class="op">=</span> np.<span class="bu">sum</span>(y_minus_t, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true"></a>    w <span class="op">-=</span> eta<span class="op">*</span>dw </span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true"></a>    b <span class="op">-=</span> eta<span class="op">*</span>db </span></code></pre></div>
<p>We can make a plot of the training loss versus the training iteration:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a>plt.plot(loss)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a>plt.xlabel(<span class="st">&quot;iteration&quot;</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a>plt.ylabel(<span class="st">&quot;loss&quot;</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true"></a>plt.show()</span></code></pre></div>
<p><img src="./myMediaFolder/6aca59c3a160a6dd4e85886bd57dc7a84dfa3ce8.png" /></p>
<p>And we can plot the regression model over the training data:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a>ymodel <span class="op">=</span> np.dot(w.transpose(), phi_range) <span class="op">+</span> b</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a>plt.plot(x[<span class="dv">0</span>], t[<span class="dv">0</span>], <span class="st">&#39;o&#39;</span>, label<span class="op">=</span><span class="st">&#39;problem data&#39;</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a>plt.plot(<span class="bu">xrange</span>[<span class="dv">0</span>], ytrue[<span class="dv">0</span>], <span class="st">&#39;r-&#39;</span>, label<span class="op">=</span><span class="st">&#39;true model&#39;</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a>plt.plot(<span class="bu">xrange</span>[<span class="dv">0</span>], ymodel[<span class="dv">0</span>], <span class="st">&#39;m-&#39;</span>, label<span class="op">=</span><span class="st">&#39;linear regression model&#39;</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true"></a>plt.xlabel(<span class="st">&quot;x&quot;</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true"></a>plt.ylabel(<span class="st">&quot;t&quot;</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true"></a>plt.legend()</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true"></a>plt.show()</span></code></pre></div>
<p><img src="./myMediaFolder/a739a0ce82c7f5dc8edb7f0162b0500ed881ae28.png" /></p>
<p>As we can observe, the linear regression model applied to the transformed data <span class="math inline">\(\boldsymbol{\phi} (x)\)</span> is able to provide a good estimation of <span class="math inline">\(t\)</span> given <span class="math inline">\(x\)</span>.</p>
<h2 id="some-words-about-the-model-complexity"><font color="#CA3532">Some words about the model complexity</font></h2>
<p>If the model is too complex or we do not have enough training data (or both things) and we train for too many iterations we may overfit the data.</p>
<p>In the following code we force this situation by:</p>
<ul>
<li><p>Reducing the number of training instances to <span class="math inline">\(10\)</span>.</p></li>
<li><p>Increasing the complexity of the attribute transformation:</p></li>
</ul>
<p><span class="math display">\[
\boldsymbol{\phi} (x) = (x^{20}, x^{19}, ..., x^{2}, x).\tag{6}
\]</span></p>
<ul>
<li>Increasing the number of training iterations.</li>
</ul>
<p>Data generation:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true"></a><span class="co"># Randomly generated problem data:</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true"></a>x <span class="op">=</span> xmin <span class="op">+</span> np.random.rand(<span class="dv">1</span>, n)<span class="op">*</span>(xmax <span class="op">-</span> xmin)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true"></a>t <span class="op">=</span> <span class="dv">4</span><span class="op">*</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> x <span class="op">+</span> <span class="dv">1</span> <span class="op">+</span> np.random.randn(n)<span class="op">*</span>noise</span></code></pre></div>
<p>Data visualization:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true"></a>plt.plot(x[<span class="dv">0</span>], t[<span class="dv">0</span>], <span class="st">&#39;o&#39;</span>, label<span class="op">=</span><span class="st">&quot;problem data&quot;</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true"></a>plt.plot(<span class="bu">xrange</span>[<span class="dv">0</span>], ytrue[<span class="dv">0</span>], <span class="st">&#39;r-&#39;</span>, label<span class="op">=</span><span class="st">&quot;true model&quot;</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true"></a>plt.xlabel(<span class="st">&quot;x&quot;</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true"></a>plt.ylabel(<span class="st">&quot;t&quot;</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true"></a>plt.legend()</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true"></a>plt.show()</span></code></pre></div>
<p><img src="./myMediaFolder/6b36e151cda7a78665aedc302e79bdc174c5a778.png" /></p>
<p>Model fitting:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true"></a><span class="co"># Generate the transformed attributes:</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true"></a>ndims <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true"></a>phi <span class="op">=</span> np.array([x[<span class="dv">0</span>]<span class="op">**</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(ndims, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>)])</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true"></a>phi_range <span class="op">=</span> np.array([<span class="bu">xrange</span>[<span class="dv">0</span>]<span class="op">**</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(ndims, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>)])</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true"></a><span class="co"># Standardize the attributes:</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true"></a>mean_phi <span class="op">=</span> phi.mean(axis<span class="op">=</span><span class="dv">1</span>)[:, <span class="va">None</span>]</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true"></a>std_phi <span class="op">=</span> phi.std(axis<span class="op">=</span><span class="dv">1</span>)[:, <span class="va">None</span>]</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true"></a>phi <span class="op">=</span> (phi<span class="op">-</span>mean_phi)<span class="op">/</span>std_phi</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true"></a>phi_range <span class="op">=</span> (phi_range<span class="op">-</span>mean_phi)<span class="op">/</span>std_phi</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true"></a><span class="co"># Initialize model parameters:</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true"></a>w <span class="op">=</span> np.random.randn(ndims, <span class="dv">1</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true"></a>b <span class="op">=</span> np.random.randn()</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true"></a><span class="co"># Set number of iterations and learning rate:</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true"></a>num_iters <span class="op">=</span> <span class="dv">1000000</span> </span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true"></a>eta <span class="op">=</span> <span class="fl">1.e-2</span>   </span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true"></a><span class="co"># Train the model:</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_iters):</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true"></a>    y <span class="op">=</span> np.dot(w.transpose(), phi) <span class="op">+</span> b</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true"></a>    y_minus_t <span class="op">=</span> y <span class="op">-</span> t</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true"></a>    dw <span class="op">=</span> np.<span class="bu">sum</span>(y_minus_t<span class="op">*</span>phi, axis<span class="op">=</span><span class="dv">1</span>)[:, <span class="va">None</span>]</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true"></a>    db <span class="op">=</span> np.<span class="bu">sum</span>(y_minus_t, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true"></a>    w <span class="op">-=</span> eta<span class="op">*</span>dw </span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true"></a>    b <span class="op">-=</span> eta<span class="op">*</span>db </span></code></pre></div>
<p>Results:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true"></a>ymodel <span class="op">=</span> np.dot(w.transpose(), phi_range) <span class="op">+</span> b</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true"></a>plt.plot(x[<span class="dv">0</span>], t[<span class="dv">0</span>], <span class="st">&#39;o&#39;</span>, label<span class="op">=</span><span class="st">&#39;problem data&#39;</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true"></a>plt.plot(<span class="bu">xrange</span>[<span class="dv">0</span>], ytrue[<span class="dv">0</span>], <span class="st">&#39;r-&#39;</span>, label<span class="op">=</span><span class="st">&#39;true model&#39;</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true"></a>plt.plot(<span class="bu">xrange</span>[<span class="dv">0</span>], ymodel[<span class="dv">0</span>], <span class="st">&#39;m-&#39;</span>, label<span class="op">=</span><span class="st">&#39;regression model&#39;</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true"></a>plt.xlabel(<span class="st">&quot;x&quot;</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true"></a>plt.ylabel(<span class="st">&quot;t&quot;</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true"></a>plt.legend()</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true"></a>plt.axis([<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">7</span>])</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true"></a>plt.show()</span></code></pre></div>
<p><img src="./myMediaFolder/3852dd3a056c571f01fca7e96e67f2b3c14f6837.png" /></p>
<p>Note that an excesive complexity makes the model adapt too much to the training data. In this situation the model will not generalize well to new, unseen data.</p>
<h2 id="a-second-example"><font color="#CA3532">A second example</font></h2>
<p>Let us consider now a classification problem in 2D, which we will try to solve using a logistic regression model. To build this model we will use the <a href="https://playground.tensorflow.org/">TensorFlow Playground</a>, an interactive tool for the visualization of neural networks.</p>
<p>In particular we will consider the <em>Circle</em> dataset, where one of the classes surrounds the other. We build a single neuron network with sigmoid activation function, which as we know is equivalent to a logistic regression model. As the model is linear and the problem is not, this model is not able to solve the problem.</p>
<p><img src="./myMediaFolder/0e3f9ec495d2c5980c71cc8fa5ae5bd90055e0b4.png" /></p>
<p>So the next step is to change the input representation by building a new attribute space. The tool lets us play with the following attributes: <span class="math inline">\(\{x_{1}^{2}, x_{2}^{2}, x_{1}x_{2}, \sin{x_{1}}, \sin{x_{2}}\}\)</span>, and we will use the following transformation:</p>
<p><span class="math display">\[
\boldsymbol{\phi} (x_{1}, x_{2}) = (x_{1}^{2}, x_{2}^{2}).\tag{7}
\]</span></p>
<p>The model is then:</p>
<p><span class="math display">\[
y = \sigma(w_{1} x_{1}^{2} + w_{2} x_{2}^{2} + b).\tag{8}
\]</span></p>
<p>After training, the model is able to correctly predict the class labels:</p>
<p><img src="./myMediaFolder/139f6f2386ddc0e5694a4be5433710bb357fd18c.png" /></p>
<p>We have seen that a linear model on a suitable attribute space is able to solve problems that are non-linear in the original data representation. The problem is now how to find this <em>suitable</em> attribute space, and one possible solution is to let the model learn it.</p>
<h2 id="let-the-model-learn-the-representation"><font color="#CA3532">Let the model learn the representation</font></h2>
<p>The idea behind neural networks is to build a complex model by connecting many artificial neurons. The neurons are usually hierarchically organized in a set of layers, in such a way that all the neurons in a layer project to all the neurons in the next layer (this kind of architecture is called a <strong>dense</strong>, or <strong>fully-connected</strong>, <strong>feed-forward</strong> neural network).</p>
<p>The first layer always corresponds to the model input (the <span class="math inline">\({\bf x}\)</span> vector), and the last layer is the model output <span class="math inline">\(y\)</span>. Besides the network may have any number of intermediate or <strong>hidden</strong> layers. All the network neurons (with the exception of those in the input layer) behave as shown above: they compute a linear combination of their inputs followed by a non-linearity.</p>
<p>As for the linear and logistic regression models, the network is trained to minimize a loss function that measures the difference between the network output <span class="math inline">\(y\)</span> and the expected target <span class="math inline">\(t\)</span>. But now the set of trainable parameters involves the weights and biases of all the network's neurons. In this way, the neurons in the first hidden layer (those which are directly connected to the input) extract a new set of simple features that make the problem easier to solve by subsequent layers. The second hidden layer combines these features into more complex ones and so forth. Finally, the output neuron simply applies a linear model on the feature space represented by the last hidden layer.</p>
<p>This way, instead of making feature engineering to find the most appropriate attributes for the problem, we are letting the neural network learn the optimal representation. In the previous example problem, let us consider again only the original variables <span class="math inline">\((x_{1}, x_{2})\)</span> and introduce a new hidden layer with <span class="math inline">\(5\)</span> neurons. We observe that this new layer is able to learn a data representation that allows to solve the problem.</p>
<p><img src="./myMediaFolder/401c22e4a21dd16873a8d758d547390c24ca0218.png" /></p>
</article>
</body>
</html>
